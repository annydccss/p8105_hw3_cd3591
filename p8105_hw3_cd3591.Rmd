---
title: "p8105_hw3_cd3591"
author: "Anny"
date: "2025-10-10"
output: github_document
---

```{r}
# set up libraries and the dataset
library(tidyverse)
library(p8105.datasets)
data("instacart")
```

# Problem 1
```{r}
# basic structure of the dataset
str(instacart)
head(instacart) # illustrative examples of observations
```
**Short Description**: The instacart dataset contains `r nrow(instacart)` observations of `r ncol(instacart)` variables, representing individual grocery orders placed through the Instacart platform. Each observation corresponds to a single product ordered by a user. Key variables include: order_id, indicating unique identifier for each order;  product_id & product_name, identifying the product ordered; aisle_id / department_id, describing where the product is found; order_hour_of_day, showing the time of order placement; order_dow, indicating the day of week (0 = Sunday).

Q1
```{r}
# How many aisles are there, and which aisles are the most items ordered from?
aisle = instacart %>% count(aisle, sort = TRUE)
n_aisle = nrow(aisle)
head(aisle, 3)
```
There are `r n_aisle` aisles in total. The most frequently ordered aisles are fresh vegetables, fresh fruits, and packaged vegetables fruits.

Q2
```{r}
aisle %>%
  filter(n > 10000) %>%  # limiting this to aisles with more than 10000 items ordered
  mutate(aisle = fct_reorder(aisle, n)) %>%
  ggplot(aes(x = n, y = aisle)) +  # Make a plot that shows the number of items ordered in each aisle
  geom_col() +
  labs(title = "Number of Items Ordered per Aisle (>10,000 orders)",
       x = "Number of Items Ordered", y = "Aisle Name")   
       # since there are long labels like “packaged vegetables fruits”, fit much better along the y-axis than rotated on the x-axis.
```

Q3
```{r}
popular_items = instacart %>%  # Make a table
  filter(aisle == "baking ingredients" |
    aisle == "dog food care" |
    aisle == "packaged vegetables fruits") %>%  
  # in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  group_by(aisle, product_name) %>%
  summarise(times_ordered = n(), .groups = "drop") %>%  # Include the number of times each item is ordered in your table
  slice_max(times_ordered, n = 3)  # showing the three most popular items

popular_items
```

Q4
```{r}
mean_hour = instacart %>%  # Make a table
  filter(product_name == "Pink Lady Apples" | 
           product_name == "Coffee Ice Cream") %>%  # at which Pink Lady Apples and Coffee Ice Cream
  group_by(product_name, order_dow) %>%  # ordered on each day of the week
  summarise(mean_hour = mean(order_hour_of_day), .groups = "drop") %>%  # showing the mean hour of the day
  pivot_wider(names_from = order_dow, values_from = mean_hour) %>%  # produce a 2 x 8 table
  rename(Sun = "0", Mon = "1", Tue = "2", Wed = "3", Thu = "4", Fri = "5", Sat = "6")

mean_hour
```

# Problem 2
```{r}
# read the dataset
zip_info = read.csv("Zip Codes.csv", na = c("NA", ".", ""))%>% 
  janitor::clean_names() # clean names
zori = read.csv("Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv", na = c("NA", ".", ""))%>% 
  janitor::clean_names() # clean names

# tidy zori
zori_new = zori %>% pivot_longer(
  x2015_01_31:x2024_08_31, 
  names_to = "date",
  values_to = "ZORI",
  names_prefix = "x"
  ) %>%  # tidy the dates
  mutate(date = ymd(str_replace_all(date, "_", "-")),
         zip_code  = as.integer(region_name)
         ) 

# collapse zip_info to one row per ZIP to avoid many-to-many
zip_info_one = zip_info %>%
  mutate(county = if_else(county == "New York", "Manhattan", county),
         zip_code = as.integer(zip_code)) %>%
  group_by(zip_code) %>%
  summarise(borough = first(county),
            neighborhood = str_c(sort(unique(neighborhood)), collapse = "; "),
            .groups = "drop") %>%
  dplyr::mutate(neighborhood = dplyr::na_if(neighborhood, ""))  # identify the empty is NA

#merge together
NY_zori = zori_new %>%
  left_join(zip_info_one,
            by = "zip_code") %>%
  dplyr::mutate(neighborhood = dplyr::na_if(neighborhood, "")) %>%  # identify the empty is NA
  arrange(zip_code, date) %>%
  relocate(zip_code, date, ZORI)
```

Q1
```{r}
ny_1501_2408 = NY_zori %>%
  filter(date >= ymd("2015-01-01"), date <= ymd("2024-08-31"))  # filter the date between January 2015 and August 2024

zip_counts = ny_1501_2408 %>%
  count(zip_code, name = "n_months")  # create the counts of zip table

n_116  = sum(zip_counts$n_months == 116)  # How many ZIP codes are observed 116 times?

n_lt10 = sum(zip_counts$n_months < 10)  #How many are observed fewer than 10 times?

# There are 116 months between January 2015 and August 2024. How many ZIP codes are observed 116 times? How many are observed fewer than 10 times? Why are some ZIP codes are observed rarely and others observed in each month?
```
There are `r n_116` ZIP codes observed 116 times and there are `r n_lt10` observed fewer than 10 times. Some ZIP codes don’t have many rental listings, like some small or mostly owner-occupied areas. When there isn’t enough data in a month, Zillow may leave it missing. By contrast, ZIP codes with consistent, high listing volume (dense residential areas) have enough data every month, so they’re observed in all 116 months.

Q2
```{r}
average_rental = NY_zori %>%
  mutate(year = year(date)) %>%
  group_by(borough, year) %>%  # showing the average rental price in each borough and year (not month)
  summarise(avg_rent = mean(ZORI, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(names_from = year, values_from = avg_rent) %>%
  arrange(borough) %>%
  knitr::kable()

average_rental
```
**Comments**: During 2015.01–2024.08, Manhattan is consistently the most expensive borough, followed by Brooklyn, Queens, and the Bronx, with Richmond slightly above the Bronx once data appear in 2020. Rents generally rose from 2015 to 2019, flattened around 2020–2021, and then rebounded sharply to new highs by 2024 in every borough. The post-2021 surge is the strongest in Manhattan, widening the gap between high- and lower-priced areas. Early years for Richmond are missing, which is why those cells show NaN.

